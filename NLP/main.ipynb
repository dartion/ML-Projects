{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Version : 3.4.4 \n",
      "SkLearn Version : 0.21.2 \n",
      "Python Version : 3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)] \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import sys \n",
    "\n",
    "\n",
    "# Uncomment the following lines to print the version of each imports above\n",
    "\n",
    "# print(\"NLTK Version : {} \".format(nltk.__version__))\n",
    "# print(\"SkLearn Version : {} \".format(sklearn.__version__))\n",
    "# print(\"Python Version : {} \".format(sys.version))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello students, how are you doing today?', 'The olypics are inspiring, and Python is awesome.', 'You look great today.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Tokenize sentances and print\n",
    "text  = \"Hello students, how are you doing today? The olypics are inspiring, and Python is awesome. You look great today.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'students', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'olypics', 'are', 'inspiring', ',', 'and', 'Python', 'is', 'awesome', '.', 'You', 'look', 'great', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words and print\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you'd\", 'no', 'should', 'll', 'over', \"you've\", 'which', 'too', 'between', 'themselves', 'that', 'while', \"didn't\", 'there', 'ourselves', 'before', 'needn', 'of', 'with', 'aren', 'did', 'we', 'been', 'until', 'wasn', 'her', 're', 'yours', 'wouldn', 'are', 'don', 'for', 'the', 'above', \"haven't\", 'it', 'some', 'nor', 'where', \"should've\", 'here', \"shan't\", 'a', 'not', 'be', 'was', 'its', 'his', 'from', 'on', \"hadn't\", 'other', 'because', 'weren', 'what', 'y', 'ma', 'my', 'does', 'during', 'their', 'out', 'at', 'more', 'mightn', \"wouldn't\", 'myself', 'having', 'both', 'mustn', 'hadn', 'hasn', \"shouldn't\", 'any', 'this', 'ain', \"you're\", 'up', \"couldn't\", 'can', 'didn', 'all', 'but', 'to', 'once', 've', \"won't\", 'your', 'again', 'few', 'me', 'itself', 'couldn', \"you'll\", 'in', 'he', 'then', \"weren't\", 'they', \"doesn't\", \"aren't\", \"mustn't\", 'shouldn', \"it's\", 'doesn', 'who', 'you', 'do', 'is', 'same', 'doing', 'won', 'by', 'off', 'such', 'just', 'i', \"hasn't\", 'him', 'and', 'hers', 'will', 'as', 'am', 'than', 'against', 'these', 'only', 'o', 'ours', \"that'll\", 'through', 's', 't', 'had', 'those', 'own', 'whom', 'further', 'our', 'has', 'an', 'herself', 'each', 'very', \"mightn't\", 'she', 'being', 'were', 'under', 'if', 'd', 'have', 'haven', 'yourself', 'now', \"wasn't\", 'when', 'why', \"isn't\", 'himself', 'below', \"don't\", 'isn', 'yourselves', 'theirs', 'after', 'or', 'how', 'shan', \"she's\", 'into', 'm', 'about', 'most', 'down', 'so', \"needn't\", 'them'}\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(set(stopwords.words('english')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'some', 'sample', 'text', ',', 'showung', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'text', ',', 'showung', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'text', ',', 'showung', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sentance = \"This is some sample text, showung off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sentance)\n",
    "\n",
    "sentance = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentance = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentance.append(w)\n",
    "\n",
    "        \n",
    "print(word_tokens)\n",
    "print(sentance)\n",
    "print(filtered_sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride\n",
      "ride\n",
      "rider\n",
      "ride\n"
     ]
    }
   ],
   "source": [
    "# Stemming words with NLTK\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['ride', 'riding', 'rider', 'rides']\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "rider\n",
      "are\n",
      "ride\n",
      "their\n",
      "hors\n",
      ",\n",
      "they\n",
      "often\n",
      "think\n",
      "of\n",
      "how\n",
      "cow\n",
      "boy\n",
      "rode\n",
      "hors\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stemming in entire sentace\n",
    "\n",
    "some_text = \"When riders are riding their horses, they often think of how cow boys rode horses.\"\n",
    "\n",
    "words = word_tokenize(some_text)\n",
    "\n",
    "for each_word in words:\n",
    "    print(ps.stem(each_word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "\n",
    "# Print a sample text\n",
    "# print(udhr.raw(\"English-Latin1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Training text\n",
    "# print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PunktSentanceToPunktSentenceTokenizer\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sample text\n",
    "\n",
    "tokenized_sample_text = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# print(tokenized_sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Define a function that will tag each tokenized word with a part of speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized_sample_text[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized_sample_text = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# Define a function that will tag each tokenized word with a part of speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized_sample_text[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            # Combine the part of speech tag with a regular expression\n",
    "            \n",
    "            # <RB.?>* = \"0 or more of anytense of adverb, followed by:\"\n",
    "            # <VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "            # <NPP>+ = \"One or more proper nounds, followed by :\"\n",
    "            # <NN>? = \"Zero or one  singular noun.\"\n",
    "        \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            \n",
    "            # Draw the chunks with nltk\n",
    "            chunked.draw()\n",
    "            \n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n"
     ]
    }
   ],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized_sample_text = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# Define a function that will tag each tokenized word with a part of speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized_sample_text[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            # Combine the part of speech tag with a regular expression\n",
    "            \n",
    "            # <RB.?>* = \"0 or more of anytense of adverb, followed by:\"\n",
    "            # <VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "            # <NPP>+ = \"One or more proper nounds, followed by :\"\n",
    "            # <NN>? = \"Zero or one  singular noun.\"\n",
    "        \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            \n",
    "            # print the nltk tree\n",
    "            for subtree in chunked.subtrees(filter=lambda t:t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            # Draw the chunks with nltk\n",
    "            # chunked.draw()\n",
    "            \n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk\n",
      "  THE/NNP\n",
      "  UNION/NNP\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP)\n",
      "(Chunk ./.)\n",
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS)\n",
      "(Chunk Congress/NNP ,/, members/NNS)\n",
      "(Chunk\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:)\n",
      "(Chunk our/PRP$ nation/NN)\n",
      "(Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "(Chunk noble/JJ dream/NN ./.)\n"
     ]
    }
   ],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized_sample_text = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# Define a function that will tag each tokenized word with a part of speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized_sample_text[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            # Combine the part of speech tag with a regular expression\n",
    "            \n",
    "            # <RB.?>* = \"0 or more of anytense of adverb, followed by:\"\n",
    "            # <VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "            # <NPP>+ = \"One or more proper nounds, followed by :\"\n",
    "            # <NN>? = \"Zero or one  singular noun.\"\n",
    "        \n",
    "        \n",
    "            # Words are unwanted are removed using inverted brackets, use | to add multiple types\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<.+>+}\n",
    "                                        } <VB.?|IN|DT|TO>+{\"\"\"\n",
    "            \n",
    "            # Removed Verbs, Prepositions, determiners , or the word \"to\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            \n",
    "            # print the nltk tree\n",
    "            for subtree in chunked.subtrees(filter=lambda t:t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            # Draw the chunks with nltk\n",
    "            # chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "process_content()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized_sample_text = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# Define a function that will tag each tokenized word with a part of speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized_sample_text[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEntity = nltk.ne_chunk(tagged, binary=False)\n",
    "             \n",
    "            namedEntity.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "process_content()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 2000 \n",
      "First Reviews: (['now', ',', 'lets', 'first', 'look', 'into', 'the', 'history', 'of', 'shark', 'films', '.', 'there', 'was', 'the', 'unforgettable', 'jaws', '.', 'the', 'exciting', 'jaws', '2', '.', 'the', 'rather', 'flaky', 'jaws', '3d', 'and', 'sometime', 'in', 'the', 'late', '90s', 'another', 'film', 'of', 'the', 'same', 'genre', 'that', 'i', 'can', \"'\", 't', 'seem', 'to', 'recall', '(', 'about', 'the', 'son', 'of', 'jaws', 'returning', 'to', 'wreak', 'revenge', 'or', 'something', 'like', 'that', ')', '.', 'now', ',', 'with', 'the', 'magic', 'of', 'cgi', ',', 'one', 'shark', 'is', 'simply', 'not', 'enough', ';', 'in', 'deep', 'blue', 'sea', ',', 'there', 'are', '3', 'big', ',', 'mean', 'and', 'really', 'smart', 'ones', '!', 'russell', 'frankiln', '(', 'jackson', ')', 'visits', 'aquatica', '(', 'a', 'sea', '-', 'bound', 'research', 'center', ')', ',', 'where', 'a', 'research', 'is', 'being', 'conducted', 'on', 'the', 'extraction', 'of', 'a', 'hormone', 'substance', 'found', 'uniquely', 'within', 'the', 'shark', 'brain', 'that', 'can', 'cure', 'and', 'reverse', 'the', 'effects', 'of', 'alzheimer', \"'\", 's', 'disease', '.', 'the', 'substance', 'is', 'small', 'in', 'quantity', 'and', 'because', 'of', 'this', ',', 'lead', 'researcher', 'dr', '.', 'susan', 'macalaester', 'genetically', 'alters', 'the', 'shark', 'dna', 'and', 'grows', 'them', 'twice', 'the', 'size', 'with', 'brains', 'as', 'big', 'as', 'humans', ',', 'naturally', 'producing', 'more', 'of', 'the', 'much', 'treasured', 'hormones', '.', 'as', 'sure', 'as', 'the', 'sun', 'sets', 'in', 'the', 'west', ',', 'a', 'shark', 'breaks', 'lose', 'and', 'wreaks', 'havoc', 'within', 'the', 'facility', 'during', 'a', 'hormone', 'extraction', 'procedure', '.', 'with', 'the', 'flexibility', 'offered', 'by', 'cgi', ',', 'the', 'sharks', 'get', 'more', 'full', '-', 'length', 'screen', 'time', 'as', 'their', 'predecessors', 'did', 'in', 'those', 'remote', '-', 'controlled', 'rubber', 'suit', 'days', '.', 'gone', 'are', 'the', 'days', 'of', 'people', 'getting', 'pulled', 'under', 'the', 'water', 'with', 'the', 'water', 'turning', 'red', 'right', 'after', '.', 'this', 'time', ',', 'we', 'get', 'to', 'see', 'the', 'entire', 'gobbling', 'action', ',', 'with', 'floating', 'limbs', 'and', 'all', '.', 'so', '-', 'so', 'acting', ',', 'expected', 'in', 'most', 'of', 'your', 'average', 'action', 'film', '.', 'jackson', '(', 'fresh', 'from', 'his', 'jedi', 'master', 'role', 'in', 'the', 'phantom', 'menace', ')', ',', 'takes', 'on', 'the', 'dark', '-', 'side', 'force', 'of', 'a', 'different', 'kind', 'and', 'fits', 'well', 'with', 'his', 'wise', '-', 'cracking', 'lines', '.', 'll', 'cool', 'j', \"'\", 's', 'cook', 'role', 'as', 'preacher', ',', 'does', 'for', 'this', 'film', 'what', 'steven', 'seagal', \"'\", 's', 'cook', 'could', 'only', 'dream', 'to', 'achieve', 'in', 'his', 'two', 'under', 'siege', 'films', '.', 'deep', 'blue', 'sea', 'does', 'not', 'offer', 'any', 'of', 'the', 'psychological', 'thrills', 'that', 'jaws', 'has', 'to', 'offer', '.', 'it', 'does', 'however', ',', 'prove', 'to', 'be', 'a', 'refreshing', 'follow', '-', 'up', 'within', 'the', 'genre', ',', 'full', 'of', 'visual', 'thrills', ',', 'suspense', 'and', 'believe', 'it', 'or', 'not', '\\x05', 'humour', '!', 'its', 'like', 'jurassic', 'park', 'under', 'water', '.', 'nothing', 'too', 'stressful', ',', 'pure', 'entertainment', '.', 'renny', 'harlin', ',', 'you', 'are', 'forgiven', '(', 'for', 'making', 'cutthroat', 'island', ')', '.'], 'pos') \n",
      "Most common words: [(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "The word happy: 215\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "\n",
    "# Build a list of documents\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents \n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents: {} '.format(len(documents)))\n",
    "print(\"First Reviews: {} \".format(documents[0]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "print(\"Most common words: {}\".format(all_words.most_common(15)))\n",
    "print(\"The word happy: {}\".format(all_words[\"happy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key is : plot \n",
      "Key is : : \n",
      "Key is : two \n",
      "Key is : teen \n",
      "Key is : couples \n",
      "Key is : go \n",
      "Key is : to \n",
      "Key is : a \n",
      "Key is : church \n",
      "Key is : party \n",
      "Key is : , \n",
      "Key is : drink \n",
      "Key is : and \n",
      "Key is : then \n",
      "Key is : drive \n",
      "Key is : . \n",
      "Key is : they \n",
      "Key is : get \n",
      "Key is : into \n",
      "Key is : an \n",
      "Key is : accident \n",
      "Key is : one \n",
      "Key is : of \n",
      "Key is : the \n",
      "Key is : guys \n",
      "Key is : dies \n",
      "Key is : but \n",
      "Key is : his \n",
      "Key is : girlfriend \n",
      "Key is : continues \n",
      "Key is : see \n",
      "Key is : him \n",
      "Key is : in \n",
      "Key is : her \n",
      "Key is : life \n",
      "Key is : has \n",
      "Key is : nightmares \n",
      "Key is : what \n",
      "Key is : ' \n",
      "Key is : s \n",
      "Key is : deal \n",
      "Key is : ? \n",
      "Key is : watch \n",
      "Key is : movie \n",
      "Key is : \" \n",
      "Key is : sorta \n",
      "Key is : find \n",
      "Key is : out \n",
      "Key is : critique \n",
      "Key is : mind \n",
      "Key is : - \n",
      "Key is : fuck \n",
      "Key is : for \n",
      "Key is : generation \n",
      "Key is : that \n",
      "Key is : touches \n",
      "Key is : on \n",
      "Key is : very \n",
      "Key is : cool \n",
      "Key is : idea \n",
      "Key is : presents \n",
      "Key is : it \n",
      "Key is : bad \n",
      "Key is : package \n",
      "Key is : which \n",
      "Key is : is \n",
      "Key is : makes \n",
      "Key is : this \n",
      "Key is : review \n",
      "Key is : even \n",
      "Key is : harder \n",
      "Key is : write \n",
      "Key is : since \n",
      "Key is : i \n",
      "Key is : generally \n",
      "Key is : applaud \n",
      "Key is : films \n",
      "Key is : attempt \n",
      "Key is : break \n",
      "Key is : mold \n",
      "Key is : mess \n",
      "Key is : with \n",
      "Key is : your \n",
      "Key is : head \n",
      "Key is : such \n",
      "Key is : ( \n",
      "Key is : lost \n",
      "Key is : highway \n",
      "Key is : & \n",
      "Key is : memento \n",
      "Key is : ) \n",
      "Key is : there \n",
      "Key is : are \n",
      "Key is : good \n",
      "Key is : ways \n",
      "Key is : making \n",
      "Key is : all \n",
      "Key is : types \n",
      "Key is : these \n",
      "Key is : folks \n",
      "Key is : just \n",
      "Key is : didn \n",
      "Key is : t \n",
      "Key is : snag \n",
      "Key is : correctly \n",
      "Key is : seem \n",
      "Key is : have \n",
      "Key is : taken \n",
      "Key is : pretty \n",
      "Key is : neat \n",
      "Key is : concept \n",
      "Key is : executed \n",
      "Key is : terribly \n",
      "Key is : so \n",
      "Key is : problems \n",
      "Key is : well \n",
      "Key is : its \n",
      "Key is : main \n",
      "Key is : problem \n",
      "Key is : simply \n",
      "Key is : too \n",
      "Key is : jumbled \n",
      "Key is : starts \n",
      "Key is : off \n",
      "Key is : normal \n",
      "Key is : downshifts \n",
      "Key is : fantasy \n",
      "Key is : world \n",
      "Key is : you \n",
      "Key is : as \n",
      "Key is : audience \n",
      "Key is : member \n",
      "Key is : no \n",
      "Key is : going \n",
      "Key is : dreams \n",
      "Key is : characters \n",
      "Key is : coming \n",
      "Key is : back \n",
      "Key is : from \n",
      "Key is : dead \n",
      "Key is : others \n",
      "Key is : who \n",
      "Key is : look \n",
      "Key is : like \n",
      "Key is : strange \n",
      "Key is : apparitions \n",
      "Key is : disappearances \n",
      "Key is : looooot \n",
      "Key is : chase \n",
      "Key is : scenes \n",
      "Key is : tons \n",
      "Key is : weird \n",
      "Key is : things \n",
      "Key is : happen \n",
      "Key is : most \n",
      "Key is : not \n",
      "Key is : explained \n",
      "Key is : now \n",
      "Key is : personally \n",
      "Key is : don \n",
      "Key is : trying \n",
      "Key is : unravel \n",
      "Key is : film \n",
      "Key is : every \n",
      "Key is : when \n",
      "Key is : does \n",
      "Key is : give \n",
      "Key is : me \n",
      "Key is : same \n",
      "Key is : clue \n",
      "Key is : over \n",
      "Key is : again \n",
      "Key is : kind \n",
      "Key is : fed \n",
      "Key is : up \n",
      "Key is : after \n",
      "Key is : while \n",
      "Key is : biggest \n",
      "Key is : obviously \n",
      "Key is : got \n",
      "Key is : big \n",
      "Key is : secret \n",
      "Key is : hide \n",
      "Key is : seems \n",
      "Key is : want \n",
      "Key is : completely \n",
      "Key is : until \n",
      "Key is : final \n",
      "Key is : five \n",
      "Key is : minutes \n",
      "Key is : do \n",
      "Key is : make \n",
      "Key is : entertaining \n",
      "Key is : thrilling \n",
      "Key is : or \n",
      "Key is : engaging \n",
      "Key is : meantime \n",
      "Key is : really \n",
      "Key is : sad \n",
      "Key is : part \n",
      "Key is : arrow \n",
      "Key is : both \n",
      "Key is : dig \n",
      "Key is : flicks \n",
      "Key is : we \n",
      "Key is : actually \n",
      "Key is : figured \n",
      "Key is : by \n",
      "Key is : half \n",
      "Key is : way \n",
      "Key is : point \n",
      "Key is : strangeness \n",
      "Key is : did \n",
      "Key is : start \n",
      "Key is : little \n",
      "Key is : bit \n",
      "Key is : sense \n",
      "Key is : still \n",
      "Key is : more \n",
      "Key is : guess \n",
      "Key is : bottom \n",
      "Key is : line \n",
      "Key is : movies \n",
      "Key is : should \n",
      "Key is : always \n",
      "Key is : sure \n",
      "Key is : before \n",
      "Key is : given \n",
      "Key is : password \n",
      "Key is : enter \n",
      "Key is : understanding \n",
      "Key is : mean \n",
      "Key is : showing \n",
      "Key is : melissa \n",
      "Key is : sagemiller \n",
      "Key is : running \n",
      "Key is : away \n",
      "Key is : visions \n",
      "Key is : about \n",
      "Key is : 20 \n",
      "Key is : throughout \n",
      "Key is : plain \n",
      "Key is : lazy \n",
      "Key is : ! \n",
      "Key is : okay \n",
      "Key is : people \n",
      "Key is : chasing \n",
      "Key is : know \n",
      "Key is : need \n",
      "Key is : how \n",
      "Key is : giving \n",
      "Key is : us \n",
      "Key is : different \n",
      "Key is : offering \n",
      "Key is : further \n",
      "Key is : insight \n",
      "Key is : down \n",
      "Key is : apparently \n",
      "Key is : studio \n",
      "Key is : took \n",
      "Key is : director \n",
      "Key is : chopped \n",
      "Key is : themselves \n",
      "Key is : shows \n",
      "Key is : might \n",
      "Key is : ve \n",
      "Key is : been \n",
      "Key is : decent \n",
      "Key is : here \n",
      "Key is : somewhere \n",
      "Key is : suits \n",
      "Key is : decided \n",
      "Key is : turning \n",
      "Key is : music \n",
      "Key is : video \n",
      "Key is : edge \n",
      "Key is : would \n",
      "Key is : actors \n",
      "Key is : although \n",
      "Key is : wes \n",
      "Key is : bentley \n",
      "Key is : seemed \n",
      "Key is : be \n",
      "Key is : playing \n",
      "Key is : exact \n",
      "Key is : character \n",
      "Key is : he \n",
      "Key is : american \n",
      "Key is : beauty \n",
      "Key is : only \n",
      "Key is : new \n",
      "Key is : neighborhood \n",
      "Key is : my \n",
      "Key is : kudos \n",
      "Key is : holds \n",
      "Key is : own \n",
      "Key is : entire \n",
      "Key is : feeling \n",
      "Key is : unraveling \n",
      "Key is : overall \n",
      "Key is : doesn \n",
      "Key is : stick \n",
      "Key is : because \n",
      "Key is : entertain \n",
      "Key is : confusing \n",
      "Key is : rarely \n",
      "Key is : excites \n",
      "Key is : feels \n",
      "Key is : redundant \n",
      "Key is : runtime \n",
      "Key is : despite \n",
      "Key is : ending \n",
      "Key is : explanation \n",
      "Key is : craziness \n",
      "Key is : came \n",
      "Key is : oh \n",
      "Key is : horror \n",
      "Key is : slasher \n",
      "Key is : flick \n",
      "Key is : packaged \n",
      "Key is : someone \n",
      "Key is : assuming \n",
      "Key is : genre \n",
      "Key is : hot \n",
      "Key is : kids \n",
      "Key is : also \n",
      "Key is : wrapped \n",
      "Key is : production \n",
      "Key is : years \n",
      "Key is : ago \n",
      "Key is : sitting \n",
      "Key is : shelves \n",
      "Key is : ever \n",
      "Key is : whatever \n",
      "Key is : skip \n",
      "Key is : where \n",
      "Key is : joblo \n",
      "Key is : nightmare \n",
      "Key is : elm \n",
      "Key is : street \n",
      "Key is : 3 \n",
      "Key is : 7 \n",
      "Key is : / \n",
      "Key is : 10 \n",
      "Key is : blair \n",
      "Key is : witch \n",
      "Key is : 2 \n",
      "Key is : crow \n",
      "Key is : 9 \n",
      "Key is : salvation \n",
      "Key is : 4 \n",
      "Key is : stir \n",
      "Key is : echoes \n",
      "Key is : 8 \n"
     ]
    }
   ],
   "source": [
    "# Use 4000 most common words as features\n",
    "\n",
    "word_features = list(all_words.keys())[:4000]\n",
    "\n",
    "# Build a find_features function that will determine which of the 4000 word features are contained in a review\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        # Build the dict\n",
    "    return features\n",
    "\n",
    "# Using a example of negative review\n",
    "\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(\"Key is : {} \".format(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(rev), category)  for (rev, category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Build Training and Testing Dataset  by splitting them using SkLearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Define seed for reproducabilty\n",
    "seed = 1\n",
    "\n",
    "# Split the data\n",
    "training, testing = model_selection.train_test_split(feature_sets, test_size=0.25, random_state=seed)\n",
    "\n",
    "print(len(training))\n",
    "print(len(testing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "model.train(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 0.824 \n"
     ]
    }
   ],
   "source": [
    "# Test on the testing dataset\n",
    "accuracy = nltk.classify.accuracy(model, testing)\n",
    "print(\"SVC Accuracy: {} \".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
